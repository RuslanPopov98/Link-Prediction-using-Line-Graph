{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d62162-cbb5-4a94-9b94-ae72fd1014e3",
   "metadata": {},
   "source": [
    "# Описание программы\n",
    "## Обозначения:\n",
    "$g$ - dgl graph\n",
    "\n",
    "$G$ - nx graph\n",
    "\n",
    "$lg$ - dgl line graoh\n",
    "\n",
    "$LG$ - nx line graph\n",
    "\n",
    "$f_{u}$- фичи вершин графа g\n",
    "\n",
    "$z_{u} = GCN(f_{u})$ - эмбеддинг вершины в основном графе\n",
    "\n",
    "$g_{uv} = FC([z_{u};z_{v}])$  - эмбеддинг ребра (u,v) как функция FC от конкатенации эмбеддингов вершин\n",
    "\n",
    "$f_{uv}^{*}$ - фичи вершин дуального графа (или ребер обычного графа)\n",
    "\n",
    "$z_{uv}^{*} = GCN(f_{uv})$ - эмбеддинг вершины дуального графа (ребра (u,v) в обычном графе)\n",
    "\n",
    "$g_{u}^{*} = AGG(FC([z_{uv};z_{ut}]))$ - агрегатор (среднее по эмбеддингам) по всем ребрам в двойственном графе, соответствующим вершине в исходном графе\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d1922-388b-4b11-8c5f-e9c36dcdac8e",
   "metadata": {},
   "source": [
    "## Этапы программы:\n",
    "    Этап 1: формирование простого графа и фичи для его вершин (на основе node2vec)\n",
    "            далаю сэмл на трейн и тест и поситив и негати выборку и далее для дуального графа использую\n",
    "            только поситив трейн часть графа; \n",
    "    Этап 2: формирование дуального графа и фичи для его вершин (на основе node2vec);\n",
    "    Этап 3: инициализирую GCN(GraphSAGE) и полносвязную сеть(FC-на выходе вектор) и обучаю эти две сети. \n",
    "            На выходе получаю обученный GCN и фичи вершин исходного графа;\n",
    "    Этап 4: Обучаю ещё одну полносвязную сеть на фичах, которые получились после GCN,\n",
    "            которая на выходе будет выдавать число(\"вероятность связи\") и тестируем модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd117f30-2d21-4541-bedb-1aa8d18f8f5b",
   "metadata": {},
   "source": [
    "Функция потерь для GCN выглядит следующим образом:\n",
    "$loss = \\alpha * MSE_{all nodes in G}(z_{u},g_{u}^{*}) + \\beta * MSE_{all edges in G}(g_{uv},z_{uv}^{*})$,\n",
    "\n",
    "где $0<= \\alpha, \\beta <=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2085da5-e0d3-4179-868b-5c4244457275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import itertools\n",
    "\n",
    "import dgl.data\n",
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.nn import SumPooling\n",
    "from dgl.nn import DenseGraphConv\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926081bd-ee99-4d16-a192-0b91f9bd8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return g.edata['score'][:, 0]\n",
    "    \n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']\n",
    "        \n",
    "class FC(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats*2, h_feats*2)\n",
    "        self.W2 = nn.Linear(h_feats*2, h_feats)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']\n",
    "        \n",
    "def positive_sample(graph, test_size=0.1):\n",
    "    u, v = graph.edges()\n",
    "    eids = np.random.permutation(np.arange(graph.number_of_edges())) #random index edges\n",
    "    test_size_idx = int(len(eids) * test_size) #size positive sample by index\n",
    "\n",
    "    test_pos_u, test_pos_v = u[eids[:test_size_idx]], v[eids[:test_size_idx]]\n",
    "    train_pos_u, train_pos_v = u[eids[test_size_idx:]], v[eids[test_size_idx:]] \n",
    "    \n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=graph.number_of_nodes())\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=graph.number_of_nodes())\n",
    "    \n",
    "    return train_pos_g, test_pos_g, eids\n",
    "\n",
    "\n",
    "def negative_sample(graph, method='kneighbors', size=None, test_size=None):   \n",
    "    new_g = graph.to_networkx()\n",
    "    adj = nx.to_numpy_array(new_g) #adjacency matrix\n",
    "    \n",
    "    if size == None:\n",
    "        size=g.number_of_nodes()\n",
    "    if test_size == None:\n",
    "        test_size=int(g.number_of_edges()*0.1)\n",
    "    \n",
    "    if method == 'dgl_example':\n",
    "        adj_neg = 1 - adj - np.eye(graph.number_of_nodes())\n",
    "        neg_u, neg_v = np.where(adj_neg != 0)\n",
    "        neg_eids = np.random.choice(len(neg_u), graph.number_of_edges() // 2) #negative sample random index\n",
    "        \n",
    "        test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "        train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "    \n",
    "    else:\n",
    "        negs_u = []\n",
    "        negs_v = []\n",
    "        negs = []\n",
    "        nnn = NearestNeighbors(n_neighbors=500, metric='cosine')\n",
    "        nnn.fit(adj)\n",
    "        res = nnn.kneighbors(return_distance=False) #top-5 nearest neightbord\n",
    "\n",
    "        for idx, i in enumerate(res):\n",
    "            for j in i:\n",
    "                if not new_g.has_edge(idx, j):\n",
    "                    negs.append([idx, j])\n",
    "\n",
    "        negs = np.array(negs)\n",
    "\n",
    "        for k in range(size):\n",
    "            temp = negs[np.random.permutation(negs.shape[0])[:graph.number_of_edges()]][0]\n",
    "            negs_u.append(temp[0])\n",
    "            negs_v.append(temp[1])\n",
    "            \n",
    "        test_neg_u, test_neg_v = negs_u[:test_size], negs_v[:test_size]\n",
    "        train_neg_u, train_neg_v = negs_u[test_size:], negs_v[test_size:]\n",
    "    \n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=graph.number_of_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=graph.number_of_nodes())\n",
    "            \n",
    "    return train_neg_g, test_neg_g\n",
    "\n",
    "\n",
    "def alternate_list(a,b):\n",
    "    c = list()\n",
    "    for x in range(len(a)):\n",
    "        c.extend([a[x], b[x]])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0676d416-3213-414b-9374-85b81d39e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 1475\n",
      "300 2950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=300, num_edges=2950,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create random graph\n",
    "# spmat = sp.rand(1000, 1000, density=0.003) # 5% nonzero entries\n",
    "# g = dgl.from_scipy(spmat)\n",
    "nx_g = nx.barabasi_albert_graph(300,5)#300, 5)\n",
    "print(nx_g.number_of_nodes(), nx_g.number_of_edges())\n",
    "g = dgl.from_networkx(nx_g, )\n",
    "G = g.to_networkx()\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae313913-133f-4bd5-a790-72893ac0fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "2708\n",
      "10556\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "# g = dgl.remove_edges(g, eids[:1000], ) #subgraph\n",
    "# g = dgl.remove_nodes(g, range(2000))\n",
    "# print(g)\n",
    "\n",
    "G = g.to_networkx()\n",
    "\n",
    "print(len(list(G.nodes())))\n",
    "print(len(list(G.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9e222c3-331e-482c-8f55-5537b992e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos_shape = [2708, 9501] ; test_pos_shape = [2708, 1055]\n",
      "train_neg_shape = [2708, 4223] ; test_neg_shape = [2708, 1055]\n"
     ]
    }
   ],
   "source": [
    "# train-test and positive-negative sampling\n",
    "train_pos_g, test_pos_g, eids = positive_sample(g)\n",
    "train_neg_g, test_neg_g = negative_sample(g, 'dgl_example')\n",
    "###########\n",
    "print('train_pos_shape =', [len(train_pos_g.nodes()), len(train_pos_g.edges()[0])], '; test_pos_shape =', [len(test_pos_g.nodes()), len(test_pos_g.edges()[0])])\n",
    "print('train_neg_shape =', [len(train_neg_g.nodes()), len(train_neg_g.edges()[0])], '; test_neg_shape =', [len(test_neg_g.nodes()), len(test_neg_g.edges()[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85685280-7711-4708-be47-68ed142dbcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n",
      "9501\n"
     ]
    }
   ],
   "source": [
    "G = train_pos_g.to_networkx()\n",
    "print(len(list(G.nodes())))\n",
    "print(len(list(G.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "324cab81-40c3-44d7-97b6-eab141e96b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec parameters \n",
    "dimensions=64\n",
    "walk_length=10\n",
    "window=20\n",
    "min_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75282fae-dbf9-465b-9663-20b59dd07664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d4c6c5b4d342f2a11f337c653b8283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:01<00:00,  9.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# create node features\n",
    "node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length)#, workers=6)\n",
    "model_n2v = node2vec.fit(window=window, min_count=min_count)\n",
    "embeddings = np.array([model_n2v.wv[x] for x in list(G.nodes())])\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "g.ndata['feat'] = embeddings                       # f_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b29ff9e1-407b-4a33-bcf9-5e8fd37ba2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.rand((g.num_nodes(), 8))\n",
    "g.ndata[\"feat\"] = torch.rand((g.num_nodes(), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "078ea938-0d43-47c9-a6fe-3f50d8a72fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(embeddings[:5] - g.ndata['feat'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee8ea2f4-be24-4f1f-860a-3bccdd3a5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size = 1055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=9501,\n",
       "      ndata_schemes={'feat': Scheme(shape=(8,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=int(g.number_of_edges()*0.1)\n",
    "print('test_size =', test_size)\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "train_g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895d3e2-bd7e-406e-a819-65641ac3f26f",
   "metadata": {},
   "source": [
    "### line graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a8098c4-193e-4e47-913d-25d808254d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffrent number of G.nodes LnxG.nodes! Used DiGraph.\n"
     ]
    }
   ],
   "source": [
    "# create line graph \n",
    "temp_G = nx.Graph()\n",
    "temp_G.add_edges_from(list(G.edges()))\n",
    "LnxG = nx.line_graph(temp_G)\n",
    "\n",
    "if len(list(LnxG.nodes())) != len(list(G.edges())):\n",
    "    print('diffrent number of G.nodes LnxG.nodes! Used DiGraph.')\n",
    "    temp_G = nx.DiGraph()\n",
    "    temp_G.add_edges_from(list(G.edges()))\n",
    "    LnxG = nx.line_graph(temp_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c64ac6a4-9ab0-480f-a6aa-17e361fd767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501 [(0, 1862), (0, 2582), (1, 2), (1, 652), (1, 654), (2, 1), (2, 1454), (2, 1666), (2, 1986), (4, 1016)]\n"
     ]
    }
   ],
   "source": [
    "print(len(list(G.edges())), sorted(list(G.edges()),reverse=False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "960e5b33-0d0e-497b-ba52-3fda882b4c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501 [(0, 1862), (0, 2582), (1, 2), (1, 652), (1, 654), (2, 1), (2, 1454), (2, 1666), (2, 1986), (4, 1016)]\n",
      "93483 [((0, 1862), (1862, 2582)), ((1862, 2582), (2582, 1862)), ((1862, 2582), (2582, 1166)), ((1862, 2582), (2582, 0)), ((0, 2582), (2582, 1862)), ((0, 2582), (2582, 1166)), ((0, 2582), (2582, 0)), ((2582, 1862), (1862, 2582)), ((2582, 1166), (1166, 2582)), ((2582, 0), (0, 1862))]\n"
     ]
    }
   ],
   "source": [
    "print(len(list(LnxG.nodes())), sorted(list(LnxG.nodes()),reverse=False)[:10])\n",
    "print(len(list(LnxG.edges())), list(LnxG.edges())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43154966-2fb3-4e14-9292-3667905d07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501 93483\n"
     ]
    }
   ],
   "source": [
    "# lg - line graph on DGL; LG - line grpah on nx\n",
    "# ф-ция, которая переназывает вершины и ребра дуального графа\n",
    "# если вершина была  (0, 633), то может стать вершиной 5, например\n",
    "def create_dgl_nx_dual_graph(line_nx_graph):\n",
    "    nodes = sorted(list(line_nx_graph.nodes()),reverse=False)\n",
    "    edges = list(line_nx_graph.edges())\n",
    "    nodes_dict = {}\n",
    "    new_u, new_v = [], []\n",
    "    \n",
    "    for idx, val in enumerate(nodes):\n",
    "        nodes_dict[val] = idx\n",
    "    \n",
    "    print(len(nodes_dict), len(edges))\n",
    "    for edge in edges:\n",
    "        new_u.append(nodes_dict[edge[0]])\n",
    "        new_v.append(nodes_dict[edge[1]])\n",
    "    \n",
    "    u = torch.tensor(new_u)\n",
    "    v = torch.tensor(new_v)\n",
    "    g = dgl.graph((u, v))\n",
    "    G = g.to_networkx()\n",
    "    return g, G\n",
    "    \n",
    "    \n",
    "lg, LG = create_dgl_nx_dual_graph(LnxG)\n",
    "dual_edges_dict = {edge: num for num, edge in enumerate(list(LG.edges()))}\n",
    "dual_nodes_dict = {node: num for num, node in enumerate(sorted(list(LnxG.nodes()),reverse=False))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bde5cb2-c9fe-4130-a2c2-e934d6d7c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "93483\n",
      "[(0, 6841), (1, 9273), (1, 9272), (1, 9271), (2, 5), (2, 8), (2, 7), (2, 6), (3, 2386), (3, 2387)]\n"
     ]
    }
   ],
   "source": [
    "#print(len(dual_nodes_dict))\n",
    "#print(len(dual_edges_dict))\n",
    "print(len(list(LG.nodes)), list(LG.nodes)[:10])\n",
    "print(len(list(LG.edges)))\n",
    "print(list(LG.edges())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20169f77-718c-499f-8092-e8e430156cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501 [tensor(0), tensor(1), tensor(2), tensor(3), tensor(4), tensor(5), tensor(6), tensor(7), tensor(8), tensor(9)]\n",
      "93483\n",
      "tensor([   0, 6841, 6841, 6841,    1,    1,    1, 9273, 9272, 9271]) tensor([   0, 6841, 6841, 6841,    1,    1,    1, 9273, 9272, 9271])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(lg.nodes())), list(lg.nodes())[:10])\n",
    "print(len(list(lg.edges())[0]))\n",
    "print(list(lg.edges())[0][:10], list(lg.edges())[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81342138-b455-487e-8ca6-3ec570929918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f110722d5be4109931d0bd5d4c7ed1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/9501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.24it/s]\n",
      "/var/folders/cm/gfrlqbxx32b2v0hgshlc8mk00000gn/T/ipykernel_33845/3184879234.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  embeddings_dual = torch.tensor(embeddings_dual)\n"
     ]
    }
   ],
   "source": [
    "# create node features to line graph\n",
    "\n",
    "# m = nn.AvgPool1d(2, stride=2)\n",
    "node2vec = Node2Vec(LnxG, dimensions=dimensions, walk_length=walk_length)\n",
    "model_n2v_dual = node2vec.fit(window=window, min_count=min_count)\n",
    "#embeddings_dual = [[alternate_list(model_n2v_dual.wv[x][0],model_n2v_dual.wv[x][1]) for x in list(LnxG.nodes)]]\n",
    "embeddings_dual = [model_n2v_dual.wv[x] for x in list(LG.nodes)]\n",
    "#embeddings_dual = m(torch.tensor(embeddings_dual))[0]\n",
    "#embeddings_dual = (torch.tensor(embeddings_dual))[0]\n",
    "embeddings_dual = torch.tensor(embeddings_dual)\n",
    "lg.ndata['feat'] = embeddings_dual                #f_uv^*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30620b4a-178c-4049-b7c2-f885055ec820",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dual = torch.rand((lg.num_nodes(), 8))\n",
    "lg.ndata[\"feat\"] = torch.rand((lg.num_nodes(), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa9ec8df-864f-4c47-ac6d-770965e4a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(embeddings_dual[:5] - lg.ndata['feat'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a032fd6c-90af-418d-8de6-0fb89b65ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=9501, num_edges=93483,\n",
      "      ndata_schemes={'feat': Scheme(shape=(8,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "train_dual_g = lg #dgl.remove_edges(dual_g, eids[:int(len(dual_eids) * 0.1)]) #subgraph\n",
    "print(train_dual_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b474b427-8b32-439b-9d49-90e96f358a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], dimensions)\n",
    "FC_net = FC(dimensions)\n",
    "###############################################################\n",
    "model_dual = GraphSAGE(train_dual_g.ndata['feat'].shape[1], dimensions)\n",
    "FC_net_dual = FC(dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "866493b7-286e-46b3-98b8-cc9bb7e4c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def g_u_star(G, LnxG, pos_score_dual):\n",
    "#     node_features = np.zeros((G.number_of_nodes(), dimensions))\n",
    "#     counts = np.zeros((G.number_of_nodes(), 1))\n",
    "\n",
    "#     for node in list(G.nodes()):\n",
    "#         for i, edge in enumerate(list(LnxG.edges())):\n",
    "#             if (node in edge[0])or(node in edge[1]):\n",
    "#                 n1, n2 = dual_nodes_dict[edge[0]], dual_nodes_dict[edge[1]]\n",
    "#                 #print(edge, n1, n2)\n",
    "#                 try:\n",
    "#                     num_embd_edge_G_star = dual_edges_dict[(n1, n2)]\n",
    "#                     node_features[node] += pos_score_dual[num_embd_edge_G_star].detach().numpy()\n",
    "#                     counts[node] += 1\n",
    "#                     #print(pos_score_dual[num_embd_edge_G_star])\n",
    "#                 except: print('NUN', edge)\n",
    "#         #print(counts)\n",
    "#         if counts[node] != 0:\n",
    "#             node_features[node]/=counts[node]\n",
    "        \n",
    "#     return torch.from_numpy(node_features)\n",
    "\n",
    "\n",
    "def sum_by_label(samples, labels, max_label):\n",
    "    weight = torch.zeros(max_label, samples.shape[0]).to(samples.device) # L, N\n",
    "    weight[labels, torch.arange(samples.shape[0])] = 1\n",
    "    #print(weight)\n",
    "    label_count = weight.sum(dim=1)\n",
    "    #print(label_count)\n",
    "    #print(samples)\n",
    "    #print(torch.mm(weight, samples))\n",
    "    return torch.mm(weight, samples), label_count # L, F\n",
    "\n",
    "# lg.ndata[\"node_from\"], lg.ndata[\"node_to\"] = g.edges()\n",
    "# line_g.edata[\"h\"] = torch.rand((line_g.num_edges(), 3))\n",
    "\n",
    "def g_u_star(embedd):\n",
    "    lg.ndata[\"node_from\"], lg.ndata[\"node_to\"] = train_g.edges()\n",
    "    src, dst = lg.edges()\n",
    "    label_from_from = lg.ndata[\"node_from\"][src]\n",
    "    label_to_from = lg.ndata[\"node_to\"][src]\n",
    "    label_from_to = lg.ndata[\"node_from\"][dst]\n",
    "    label_to_to = lg.ndata[\"node_to\"][dst]\n",
    "    \n",
    "    label_from_from_sum, label_count_1 = sum_by_label(embedd, label_from_from, train_g.num_nodes())\n",
    "    label_to_from_sum, label_count_2 = sum_by_label(embedd, label_to_from, train_g.num_nodes())\n",
    "    label_from_to_sum, label_count_3 = sum_by_label(embedd, label_from_to, train_g.num_nodes())\n",
    "    label_to_to_sum, label_count_4 = sum_by_label(embedd, label_to_to, train_g.num_nodes())\n",
    "    \n",
    "    if torch.all(torch.eq(label_from_from_sum,label_to_from_sum)):\n",
    "        label_to_from_sum=0\n",
    "    elif torch.all(torch.eq(label_from_from_sum,label_from_to_sum)):\n",
    "        label_from_to_sum=0\n",
    "    elif torch.all(torch.eq(label_from_from_sum,label_to_to_sum)):\n",
    "        label_to_to_sum=0\n",
    "    elif torch.all(torch.eq(label_to_from_sum,label_from_to_sum)):\n",
    "        label_from_to_sum=0\n",
    "    elif torch.all(torch.eq(label_to_from_sum,label_to_to_sum)):\n",
    "        label_to_to_sum=0\n",
    "    else: label_to_to_sum=0\n",
    "    \n",
    "    label_count = label_count_1 + label_count_2 + label_count_3 + label_count_4\n",
    "    label_sum = (label_from_from_sum + label_to_from_sum + label_from_to_sum + label_to_to_sum)\n",
    "    for i in range(len(label_count)):\n",
    "        if label_count[i] != 0:\n",
    "            label_sum[i]/=label_count[i]\n",
    "    \n",
    "    return label_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4218b5a-f237-4850-8a3a-d8571f842f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7609b7fa-7cb4-4a75-8649-aaa2c400c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    1,  ..., 2707, 2707, 2707])\n",
      "tensor([1862, 2582,    2,  ...,  598, 1473, 2706])\n"
     ]
    }
   ],
   "source": [
    "lg.ndata[\"node_from\"], lg.ndata[\"node_to\"] = train_g.edges()\n",
    "print(lg.ndata[\"node_from\"])\n",
    "print(lg.ndata[\"node_to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ce21711-2b5a-45a0-a90f-0532b1e3c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_for_train_SAGE(alpha, beta, z_u, g_u_star, g_uv, z_uv_star):\n",
    "    res1 = alpha*((z_u - g_u_star)**2).mean()\n",
    "    res2 = beta*((g_uv - z_uv_star)**2).mean()\n",
    "    return res1 + res2 \n",
    "    # return alfa*((z_u - g_u_star)**2).mean() + beta*((g_uv - z_uv_star)**2).mean()\n",
    "    #return alfa*F.binary_cross_entropy_with_logits(z_u, g_u_star)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6352b08-d618-46f3-8165-01b9873ddd17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/opt/anaconda3/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  0%|          | 1/200 [00:09<31:50,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.8795753717422485, time_g_u_s: 5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [03:07<26:12,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 20, loss: 0.012364903464913368, time_g_u_s: 81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [06:01<23:07,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 40, loss: 0.0032807348761707544, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [08:56<20:14,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 60, loss: 0.0015658987686038017, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [11:50<17:17,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 80, loss: 0.0006501047173514962, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [14:47<14:25,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 100, loss: 0.00041474122554063797, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [17:43<11:33,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 120, loss: 0.0003138066967949271, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [36:13<1:42:33, 104.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 140, loss: 0.00026178115513175726, time_g_u_s: 1016 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [39:04<05:28,  8.42s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 160, loss: 0.0002155122929252684, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 181/200 [41:51<02:38,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 180, loss: 0.00018874007218983024, time_g_u_s: 80 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [44:29<00:00, 13.35s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), FC_net.parameters()), lr=0.01)\n",
    "optimizer_d = torch.optim.Adam(itertools.chain(model_dual.parameters(), FC_net_dual.parameters()), lr=0.01)\n",
    "\n",
    "all_logits, diff = [], 0\n",
    "alpha, beta = 0.5, 1.\n",
    "\n",
    "for e in tqdm(range(200)):\n",
    "    h = model(train_g, embeddings)                              #z_u\n",
    "    h_dual = model_dual(train_dual_g, embeddings_dual)          #z_uv^*\n",
    "    \n",
    "    pos_score = FC_net(train_g, h)                              #g_uv\n",
    "    #neg_score = pred(train_neg_g, h)                           #g_uv -\n",
    "    pos_score_dual = FC_net_dual(train_dual_g, h_dual)          #g_u^*\n",
    "    #neg_score_dual = pred(train_neg_dual_g, h_dual)            #g_u^* -\n",
    "    start = time.time()\n",
    "    #g_u_s = g_u_star(G, LnxG, pos_score_dual)\n",
    "    g_u_s = g_u_star(pos_score_dual)\n",
    "    end = time.time()\n",
    "    diff += int(end - start)\n",
    "    loss = compute_loss_for_train_SAGE(alpha, beta, h, g_u_s, pos_score, h_dual)\n",
    "    \n",
    "    # print(h.shape)\n",
    "    # print(h_dual.shape)\n",
    "    # print(pos_score.shape)\n",
    "    # print(pos_score_dual.shape)\n",
    "    # print(g_u_s.shape)\n",
    "    # print(loss, F.mse_loss(h, g_u_s), F.mse_loss(pos_score, h_dual))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        print('In epoch {}, loss: {}, time_g_u_s: {} s'.format(e, loss, diff))\n",
    "        diff = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22b25356-f9af-4909-8cbb-f1bf0d008def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a51fd-aa69-4c33-bdcf-be7ba22979ad",
   "metadata": {},
   "source": [
    "# LP Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "20275a42-4324-4dd2-b357-1e5f731ea9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = embeddings\n",
    "#h_16 = h\n",
    "# h_16.shape\n",
    "h_temp = h\n",
    "#h = torch.rand((train_g.num_nodes(), 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "06b96a65-e8ed-4839-abaf-b10cd8308363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 64])\n",
      "torch.Size([2708, 64])\n"
     ]
    }
   ],
   "source": [
    "print(h.shape)\n",
    "print(h_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "313e3995-6245-4dc5-8895-c117bae728b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])    \n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "    #return ((scores - labels)**2).mean() \n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).detach().numpy()\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1e87e2d9-ff74-47eb-96e5-e8c0bad94195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lp = GraphSAGE(train_g.ndata['feat'].shape[1], 20)\n",
    "pred_lp = MLPPredictor(dimensions)\n",
    "#pred_lp = DotPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e7f1d914-2e9f-474c-a812-67785a784425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=2708, num_edges=9501,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "Graph(num_nodes=2708, num_edges=4223,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "print(train_pos_g)\n",
    "print(train_neg_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2d7b9ee6-510d-4247-907f-a86eb0808d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.7098416686058044\n",
      "In epoch 10, loss: 0.6181922554969788\n",
      "In epoch 20, loss: 0.61922287940979\n",
      "In epoch 30, loss: 0.6182011961936951\n",
      "In epoch 40, loss: 0.614793062210083\n",
      "In epoch 50, loss: 0.612347424030304\n",
      "In epoch 60, loss: 0.609924852848053\n",
      "In epoch 70, loss: 0.6073346138000488\n",
      "In epoch 80, loss: 0.6047480702400208\n",
      "In epoch 90, loss: 0.602157711982727\n",
      "In epoch 100, loss: 0.5995835661888123\n",
      "In epoch 110, loss: 0.5970654487609863\n",
      "In epoch 120, loss: 0.5946351885795593\n",
      "In epoch 130, loss: 0.5923190116882324\n",
      "In epoch 140, loss: 0.5900970101356506\n",
      "In epoch 150, loss: 0.5879579782485962\n",
      "In epoch 160, loss: 0.5858705043792725\n",
      "In epoch 170, loss: 0.5838161110877991\n",
      "In epoch 180, loss: 0.5818058848381042\n",
      "In epoch 190, loss: 0.5798860788345337\n",
      "train AUC 0.5\n",
      "test AUC 0.6050870375777723\n"
     ]
    }
   ],
   "source": [
    "optimizer_lp = torch.optim.Adam(pred_lp.parameters(), lr=0.01)\n",
    "\n",
    "all_logits = []\n",
    "for e in range(200):\n",
    "    model.eval()\n",
    "    h = h.detach()\n",
    "    #h_after_GCN = model(train_g, embeddings)  #train_g.ndata['feat'])\n",
    "    pos_score = pred_lp(train_pos_g, h)\n",
    "    neg_score = pred_lp(train_neg_g, h)\n",
    "    loss_lp = compute_loss(pos_score, neg_score)\n",
    "    \n",
    "    #print(loss_lp)\n",
    "    \n",
    "    optimizer_lp.zero_grad()\n",
    "    loss_lp.backward()\n",
    "    optimizer_lp.step()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss_lp))\n",
    "\n",
    "h = h.detach()\n",
    "pos_score = pred_lp(train_pos_g, h)\n",
    "neg_score = pred_lp(train_pos_g, h)\n",
    "print('train AUC', compute_auc(pos_score, neg_score))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "with torch.no_grad():\n",
    "    h = h.detach()\n",
    "    pos_score = pred_lp(test_pos_g, h)\n",
    "    neg_score = pred_lp(test_neg_g, h)\n",
    "    print('test AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f15a8ba9-27e6-4318-8f92-7e7bf1cf5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e43c614e-94de-4d1f-973b-15b885aa8ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha, beta = 0.5 , 1.0 -> AUC 0.6050870375777723\n"
     ]
    }
   ],
   "source": [
    "print('alpha, beta =', alpha, ',',beta, '-> AUC', compute_auc(pos_score, neg_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58984145-2c6b-4f85-afd6-953039659008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha, beta = 0.5, 1.0 -> AUC 0.55\n",
    "# alpha, beta = 0.25, 1.0 -> AUC 0.525\n",
    "# alpha, beta = 0.1, 1.0 -> AUC 0.5\n",
    "\n",
    "# alpha, beta = 0.9, 0.9 -> AUC 0.48\n",
    "# alpha, beta = 1.0, 0.5 -> AUC 0.48\n",
    "# alpha, beta = 1.0, 0.25 -> AUC 0.45\n",
    "\n",
    "# alpha, beta = 0.5, 0.9 -> AUC 0.54\n",
    "# alpha, beta = 0.5, 0.5 -> AUC 0.59\n",
    "# alpha, beta = 0.1, 0.5 -> AUC 0.29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25159d-49ff-4aee-974d-0c507ab3f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha, beta = 0.5, 1. -> AUC 0.7399\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857339f9-2ad7-4106-bb0d-aeef4357f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph: num_nodes=1000, num_edges=3000\n",
    "# alpha, beta = 0.4, 1.0 -> AUC 0.50885\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
